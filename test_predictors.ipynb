{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from decision_tree import calculate_information_gain\n",
    "#from decision_tree import decision_tree_train\n",
    "#from decision_tree import decision_tree_predict\n",
    "#from decision_tree import recursive_tree_train\n",
    "from scipy.io import loadmat\n",
    "from naive_bayes import naive_bayes_train, naive_bayes_predict\n",
    "from load_all_data import *\n",
    "from manipulate_data_nb import *\n",
    "from manipulate_data_mlp import *\n",
    "from manipulate_data_svm import *\n",
    "from manipulate_data_rvm import *\n",
    "from crossval import cross_validate\n",
    "from mlp import mlp_train, mlp_predict, logistic, nll\n",
    "from kernelsvm import kernel_svm_train, kernel_svm_predict\n",
    "from skrvm import RVC\n",
    "from sklearn.datasets import load_iris\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_size = [25, 50, 75, 100, 125, 150]\n",
    "#feature_size = [25]\n",
    "\n",
    "nb_test_accuracy1 = [None] * len(feature_size)\n",
    "nb_train_accuracy1 = [None] * len(feature_size)\n",
    "\n",
    "mlp_test_accuracy1 = [None] * len(feature_size)\n",
    "mlp_train_accuracy1 = [None] * len(feature_size)\n",
    "\n",
    "svm_test_accuracy1 = [None] * len(feature_size)\n",
    "svm_train_accuracy1 = [None] * len(feature_size)\n",
    "\n",
    "rvm_test_accuracy1 = [None] * len(feature_size)\n",
    "rvm_train_accuracy1 = [None] * len(feature_size)\n",
    "\n",
    "\n",
    "\n",
    "X = [None] * len(feature_size)\n",
    "y = [None] * len(feature_size)\n",
    "\n",
    "for i in range(len(feature_size)):\n",
    "    X[i], y[i] = load_all_data(feature_size[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature size: 25\n",
      "Naive Bayes training accuracy: 0.778021\n",
      "Naive Bayes testing accuracy: 0.777632\n",
      "Feature size: 50\n",
      "Naive Bayes training accuracy: 0.792469\n",
      "Naive Bayes testing accuracy: 0.789912\n",
      "Feature size: 75\n",
      "Naive Bayes training accuracy: 0.808231\n",
      "Naive Bayes testing accuracy: 0.804386\n",
      "Feature size: 100\n",
      "Naive Bayes training accuracy: 0.822023\n",
      "Naive Bayes testing accuracy: 0.821930\n",
      "Feature size: 125\n",
      "Naive Bayes training accuracy: 0.865368\n",
      "Naive Bayes testing accuracy: 0.868860\n",
      "Feature size: 150\n",
      "Naive Bayes training accuracy: 0.868214\n",
      "Naive Bayes testing accuracy: 0.870175\n"
     ]
    }
   ],
   "source": [
    "nb_params = { 'alpha': 1.0 }\n",
    "\n",
    "for i in range(len(feature_size)):\n",
    "    num_words_nb, num_training_nb, num_testing_nb, train_data_nb, test_data_nb, train_labels_nb, test_labels_nb = manipulate_data_nb(copy.deepcopy(X[i]), copy.deepcopy(y[i]))\n",
    "    \n",
    "    #print(\"test_labels\", test_labels_nb);\n",
    "    #print(\"teat_data\", test_data_nb[0:20,0:20])                                                                                                                                 \n",
    "    \n",
    "    nb_model = naive_bayes_train(train_data_nb, train_labels_nb, nb_params)\n",
    "    \n",
    "    print(\"Feature size: %d\" % feature_size[i])\n",
    "    \n",
    "    nb_train_predictions = naive_bayes_predict(train_data_nb, nb_model)\n",
    "    nb_train_accuracy = np.mean(nb_train_predictions == train_labels_nb)\n",
    "    print(\"Naive Bayes training accuracy: %f\" % nb_train_accuracy)\n",
    "    \n",
    "    nb_test_predictions = naive_bayes_predict(test_data_nb, nb_model)\n",
    "    nb_test_accuracy = np.mean(nb_test_predictions == test_labels_nb)\n",
    "    print(\"Naive Bayes testing accuracy: %f\" % nb_test_accuracy)\n",
    "    \n",
    "    nb_test_accuracy1[i] = nb_test_accuracy\n",
    "    nb_train_accuracy1[i] = nb_train_accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature size: 25\n",
      "MLP had test accuracy 0.782456\n",
      "MLP had train accuracy 0.791156\n",
      "with structure [3] and lambda = 0.010000\n",
      "Feature size: 50\n",
      "MLP had test accuracy 0.643421\n",
      "MLP had train accuracy 0.653240\n",
      "with structure [3] and lambda = 0.010000\n",
      "Feature size: 75\n",
      "MLP had test accuracy 0.888596\n",
      "MLP had train accuracy 0.882881\n",
      "with structure [3] and lambda = 0.010000\n",
      "Feature size: 100\n",
      "MLP had test accuracy 0.643421\n",
      "MLP had train accuracy 0.653240\n",
      "with structure [3] and lambda = 0.100000\n",
      "Feature size: 125\n",
      "MLP had test accuracy 0.884649\n",
      "MLP had train accuracy 0.890762\n",
      "with structure [3] and lambda = 0.100000\n",
      "Feature size: 150\n",
      "MLP had test accuracy 0.913596\n",
      "MLP had train accuracy 0.924256\n",
      "with structure [3] and lambda = 0.100000\n"
     ]
    }
   ],
   "source": [
    "num_folds = 4\n",
    "structures = [[3]]\n",
    "lambda_vals = [0.01, 0.1]\n",
    "params = {\n",
    "    'max_iter': 10000,\n",
    "    #'max_iter': 100,\n",
    "    'squash_function': logistic,\n",
    "    'loss_function': nll\n",
    "}\n",
    "    \n",
    "best_params = []\n",
    "best_score = 0\n",
    "for i in range(len(feature_size)):\n",
    "    num_words_mlp, num_training_mlp, num_testing_mlp, train_data_mlp, test_data_mlp, train_labels_mlp, test_labels_mlp = manipulate_data_mlp(copy.deepcopy(X[i]), copy.deepcopy(y[i]))\n",
    "    #print(\"test_labels\", test_labels_mlp[0:20]);\n",
    "    #print(\"teat_data\", test_data_mlp[0:20,0:20]) \n",
    "            \n",
    "    for j in range(len(structures)):\n",
    "        for k in range(len(lambda_vals)):\n",
    "            params['num_hidden_units']= structures[j]\n",
    "            params['lambda'] = lambda_vals[k]\n",
    "            #print(\"lambda\", lambda_vals[k]);\n",
    "            #print(\"structure\", structures[j]);\n",
    "            \n",
    "            cv_score, models = cross_validate(mlp_train, mlp_predict, train_data_mlp, train_labels_mlp, num_folds, params)\n",
    "        \n",
    "            #print(\"cv_score\", cv_score);\n",
    "        \n",
    "            if cv_score > best_score:\n",
    "                best_score = cv_score\n",
    "                best_params = copy.copy(params)\n",
    "                \n",
    "    print(\"Feature size: %d\" % feature_size[i])\n",
    "               \n",
    "    mlp_model = mlp_train(train_data_mlp, train_labels_mlp, best_params)\n",
    "    predictions, _, _, _ = mlp_predict(test_data_mlp, mlp_model)\n",
    "    test_accuracy = np.mean(predictions == test_labels_mlp)\n",
    "    \n",
    "    print(\"MLP had test accuracy %f\" % (test_accuracy))\n",
    "    \n",
    "    predictions, _, _, _ = mlp_predict(train_data_mlp, mlp_model)\n",
    "    train_accuracy = np.mean(predictions == train_labels_mlp)\n",
    "    print(\"MLP had train accuracy %f\" % (train_accuracy))\n",
    "    print(\"with structure %s and lambda = %f\" % (repr(best_params['num_hidden_units']), best_params['lambda']))\n",
    "    \n",
    "    mlp_test_accuracy1[i] = test_accuracy\n",
    "    mlp_train_accuracy1[i] = train_accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv_score 0.6532399299474606\n",
      "cv_score 0.6532399299474606\n",
      "cv_score 0.6532399299474606\n",
      "cv_score 0.6532399299474606\n",
      "cv_score 0.6532399299474606\n",
      "cv_score 0.6532399299474606\n",
      "cv_score 0.6532399299474606\n",
      "cv_score 0.6532399299474606\n",
      "cv_score 0.6532399299474606\n",
      "cv_score 0.6532399299474606\n",
      "cv_score 0.6532399299474606\n",
      "cv_score 0.6532399299474606\n",
      "cv_score 0.6532399299474606\n",
      "cv_score 0.6532399299474606\n",
      "cv_score 0.6532399299474606\n",
      "cv_score 0.6532399299474606\n",
      "cv_score 0.7101576182136602\n",
      "cv_score 0.7602889667250439\n",
      "cv_score 0.8012259194395797\n",
      "cv_score 0.7950963222416813\n",
      "cv_score 0.8134851138353765\n",
      "cv_score 0.7114711033274956\n",
      "cv_score 0.6532399299474606\n",
      "cv_score 0.6532399299474606\n",
      "cv_score 0.6532399299474606\n",
      "cv_score 0.6532399299474606\n",
      "cv_score 0.6532399299474606\n",
      "cv_score 0.6532399299474606\n",
      "cv_score 0.6532399299474606\n",
      "cv_score 0.6532399299474606\n",
      "cv_score 0.6532399299474606\n",
      "cv_score 0.7105954465849387\n",
      "cv_score 0.7589754816112084\n",
      "cv_score 0.7968476357267951\n",
      "cv_score 0.7894045534150613\n",
      "cv_score 0.8073555166374781\n",
      "cv_score 0.7583187390542907\n",
      "cv_score 0.7116900175131349\n",
      "cv_score 0.7116900175131349\n",
      "cv_score 0.6532399299474606\n",
      "cv_score 0.6532399299474606\n",
      "cv_score 0.6532399299474606\n",
      "cv_score 0.6532399299474606\n",
      "cv_score 0.6532399299474606\n",
      "cv_score 0.6532399299474606\n",
      "cv_score 0.6532399299474606\n",
      "cv_score 0.7105954465849387\n",
      "cv_score 0.7587565674255692\n",
      "cv_score 0.75284588441331\n",
      "cv_score 0.7968476357267951\n",
      "cv_score 0.8066987740805605\n",
      "cv_score 0.7510945709281962\n",
      "cv_score 0.6532399299474606\n",
      "cv_score 0.6532399299474606\n",
      "cv_score 0.7114711033274956\n",
      "cv_score 0.6532399299474606\n",
      "cv_score 0.6532399299474606\n",
      "cv_score 0.6532399299474606\n",
      "cv_score 0.6532399299474606\n",
      "cv_score 0.6532399299474606\n",
      "Feature size: 25\n",
      "RBF SVM had test accuracy 0.871930\n",
      "RBF SVM had train accuracy 0.923599\n",
      "with C = 2.154435, sigma = 2.564286\n",
      "cv_score 0.6532399299474606\n",
      "cv_score 0.6532399299474606\n",
      "cv_score 0.6532399299474606\n",
      "cv_score 0.6532399299474606\n",
      "cv_score 0.6532399299474606\n",
      "cv_score 0.6532399299474606\n",
      "cv_score 0.6532399299474606\n",
      "cv_score 0.6532399299474606\n",
      "cv_score 0.6532399299474606\n",
      "cv_score 0.6532399299474606\n",
      "cv_score 0.6532399299474606\n",
      "cv_score 0.6532399299474606\n",
      "cv_score 0.6532399299474606\n",
      "cv_score 0.6532399299474606\n",
      "cv_score 0.6532399299474606\n",
      "cv_score 0.6591506129597198\n",
      "cv_score 0.6803852889667251\n",
      "cv_score 0.6972416812609458\n",
      "cv_score 0.7554728546409807\n",
      "cv_score 0.7926882661996498\n",
      "cv_score 0.8178633975481612\n",
      "cv_score 0.8415061295971978\n",
      "cv_score 0.8596760070052539\n",
      "cv_score 0.9036777583187391\n",
      "cv_score 0.9380472854640981\n",
      "cv_score 0.8583625218914186\n",
      "cv_score 0.7283274956217163\n",
      "cv_score 0.8084500875656743\n",
      "cv_score 0.875\n"
     ]
    }
   ],
   "source": [
    "num_folds = 4\n",
    "c_vals = 10 ** np.linspace(-1, 3, 4)\n",
    "sigmas = np.linspace(0.1, 7, 15)\n",
    "#sigmas = [4.5]\n",
    "best_params_svm = {\n",
    "                    'kernel': 'rbf',\n",
    "                    'C': c_vals[0],\n",
    "                    'sigma': sigmas[0]\n",
    "                  }\n",
    "best_score = 0\n",
    "\n",
    "for i in range(len(feature_size)):\n",
    "    num_words_svm, num_training_svm, num_testing_svm, train_data_svm, test_data_svm, train_labels_svm, test_labels_svm = manipulate_data_svm(copy.deepcopy(X[i]), copy.deepcopy(y[i]))\n",
    "\n",
    "    #print(\"test_labels\", test_labels_svm[0:20]);\n",
    "    #print(\"teat_data\", test_data_svm[0:20,0:20]);                                                                                                                                          \n",
    "\n",
    "    for j in range(len(c_vals)):\n",
    "        for k in range(len(sigmas)):\n",
    "            params = {\n",
    "                'kernel': 'rbf',\n",
    "                'C': c_vals[j],\n",
    "                'sigma': sigmas[k]\n",
    "            }\n",
    "            \n",
    "            cv_score, _ = cross_validate(kernel_svm_train, kernel_svm_predict, train_data_svm, train_labels_svm, num_folds, params)\n",
    "        \n",
    "            print(\"cv_score\", cv_score);\n",
    "            #print(j, \" | \", k);\n",
    "        \n",
    "            if cv_score > best_score:\n",
    "                best_score = cv_score\n",
    "                best_params_svm['kernel'] = params['kernel']\n",
    "                best_params_svm['C'] = params['C']\n",
    "                best_params_svm['sigma'] = params['sigma']\n",
    "    \n",
    "    print(\"Feature size: %d\" % feature_size[i])\n",
    "                \n",
    "    rbf_svm_model = kernel_svm_train(train_data_svm, train_labels_svm, best_params_svm)\n",
    "    predictions, _ = kernel_svm_predict(test_data_svm, rbf_svm_model)\n",
    "    test_accuracy = np.mean(predictions == test_labels_svm)\n",
    "    \n",
    "    print(\"RBF SVM had test accuracy %f\" % (test_accuracy))\n",
    "    \n",
    "    predictions, _ = kernel_svm_predict(train_data_svm, rbf_svm_model)\n",
    "    train_accuracy = np.mean(predictions == train_labels_svm)\n",
    "    \n",
    "    print(\"RBF SVM had train accuracy %f\" % (train_accuracy))\n",
    "    print(\"with C = %f, sigma = %f\" % (best_params_svm['C'], best_params_svm['sigma']))\n",
    "    \n",
    "    svm_test_accuracy1[i] = test_accuracy\n",
    "    svm_train_accuracy1[i] = train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install https://github.com/JamesRitchie/scikit-rvm/archive/master.zip\n",
    "\n",
    "for i in range(len(feature_size)):\n",
    "    #print(\"X[i]\", X[i].shape)\n",
    "    #print(\"Y[i]\", y[i].shape)\n",
    "    num_words_rvm, num_training_rvm, num_testing_rvm, train_data_rvm, test_data_rvm, train_labels_rvm, test_labels_rvm = manipulate_data_rvm(copy.deepcopy(X[i]), copy.deepcopy(y[i]))\n",
    "    #print(\"train_data_rvm\", train_data_rvm.shape)\n",
    "    #print(\"train_label_rvm\", train_labels_rvm.shape)\n",
    "    clf = RVC(kernel='rbf', n_iter=10, n_iter_posterior=10, threshold_alpha=10000.0, verbose=False)\n",
    "    clf.fit(train_data_rvm, train_labels_rvm)\n",
    "    test_accuracy = clf.score(test_data_rvm, test_labels_rvm)\n",
    "    print(\"Feature size: %d\" % feature_size[i])\n",
    "    print(\"RBF RVM had test accuracy %f\" % (test_accuracy))\n",
    "    train_accuracy = clf.score(train_data_rvm, train_labels_rvm)\n",
    "    print(\"RBF RVM had train accuracy %f\" % (train_accuracy))\n",
    "    \n",
    "    rvm_test_accuracy1[i] = test_accuracy\n",
    "    rvm_train_accuracy1[i] = train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(feature_size, nb_test_accuracy1, label='NB')\n",
    "plt.plot(feature_size, mlp_test_accuracy1, label='MLP')\n",
    "plt.plot(feature_size, svm_test_accuracy1, label='SVM')\n",
    "plt.plot(feature_size, rvm_test_accuracy1, label='RVM')\n",
    "\n",
    "plt.xlabel('Number of words')\n",
    "plt.ylabel('Spam Precision')\n",
    "\n",
    "plt.title(\"Spam precision on testing data under different feature size\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "#plt.plot(a1, b1, 'ro-', label='b1', a1, b2, 'bs-', label='b2', a1, b3, 'g^-', label='b3', a1, b4, 'y*-', label='b4')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(feature_size, nb_train_accuracy1, label='NB')\n",
    "plt.plot(feature_size, mlp_train_accuracy1, label='MLP')\n",
    "plt.plot(feature_size, svm_train_accuracy1, label='SVM')\n",
    "plt.plot(feature_size, rvm_train_accuracy1, label='RVM')\n",
    "\n",
    "plt.xlabel('Number of words')\n",
    "plt.ylabel('Spam Precision')\n",
    "\n",
    "plt.title(\"Spam precision on training data under different feature size\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "#plt.plot(a1, b1, 'ro-', label='b1', a1, b2, 'bs-', label='b2', a1, b3, 'g^-', label='b3', a1, b4, 'y*-', label='b4')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
